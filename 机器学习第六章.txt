决策树
    分类任务,回归任务,多输出任务
    对复杂数据集进行拟合
    随机森林组成部分

开始预测
估计分类概率
CART训练算法
    分裂回归树(Classification And Regression Tree) 增长树
        贪婪算法: 搜索最高级别的最佳分割方式.不检查分割是否能在几个级别中的全部分割可能中找到最佳方法.不保证全局最佳.

计算复杂度
    遍历决策树,通常近似左右平衡,需要O(log(m)/log(2))个节点,有O(nmlog(m))的训练复杂度

基尼不纯度或是信息熵
    熵的减少通常成为信息增益
    基尼指数趋于在输的分支中将最多的类隔离出来,而熵指数趋向于产生略微平衡一些的决策树模型

正则化超参数
    决策树几乎不对训练数据做任何假设
    DecisionTreeClassifier
        min_samples_split 节点在被分裂之前必须具有的最小样本
        min_sample_leaf 叶节点必须具有的最小样本数
        min_weight_fraction_leaf 加权总数的一小部分实例
        max_leaf_nodes 叶节点的最大数量
        max_features 在每个节点被评估是否分裂的时候,具有的最大特征数量
        增加min_* hyperparameters或者减少 max_* hyperparameters会使模型正则化

回归
    决策树也能执行回归任务

不稳定性
    优点:
        易于理解和解释,易于使用且功能丰富和强大
    限制:
        决策树很喜欢设定正交化的决策边界,对训练数据集的旋转很敏感